context_tools.py

from puddle_server.mcp import mcp
from puddle_server.utils import run_pg_sql, get_embedding
from typing import Optional, List

# ==========================================
# HELPER FORMATTERS
# ==========================================

def format_vendor_str(v: dict) -> str:
    """Helper to format a single vendor record."""
    loc = [v.get('city'), v.get('region'), v.get('country')]
    location_str = ", ".join(filter(None, loc))
    
    return (
        f"VENDOR: {v['name']}\n"
        f" - Type: {v.get('organization_type', 'N/A')} (Founded: {v.get('founded_year', 'N/A')})\n"
        f" - Industry: {v.get('industry_focus', 'N/A')}\n"
        f" - Location: {location_str}\n"
        f" - Description: {v.get('description', 'No description available.')}"
    )

def format_dataset_str(d: dict, score: float = None) -> str:
    """Helper to format a single dataset record."""
    score_str = f" (Match Score: {score:.2f})" if score else ""
    
    return (
        f"DATASET: {d['title']}{score_str}\n"
        f" - ID: {d['id']}\n"
        f" - Vendor: {d.get('vendor_name', 'Unknown')}\n"
        f" - Domain: {d.get('domain', 'N/A')} | Pricing: {d.get('pricing_model', 'N/A')}\n"
        f" - Description: {d.get('description', 'No description.')}"
    )

# ==========================================
# VENDOR TOOLS
# ==========================================

@mcp.tool(
    description="Search for data vendors (companies) by name or industry. Use this to find who is selling data."
)
def search_vendors(query: str, limit: int = 5) -> str:
    """
    Search for vendors by name or industry focus using a partial match.
    
    Args:
        query: The search term (e.g., "Healthcare", "Global Analytics", "Finance").
        limit: The maximum number of vendors to return (default: 5).

    Returns:
        A formatted string list of vendors matching the criteria.
    """
    sql = """
        SELECT 
            id, name, industry_focus, description, 
            country, region, city, organization_type, founded_year
        FROM vendors
        WHERE 
            name ILIKE %s OR 
            industry_focus ILIKE %s
        LIMIT %s;
    """
    search_term = f"%{query}%"
    results = run_pg_sql(sql, (search_term, search_term, limit))
    
    if not results:
        return "No vendors found matching your criteria."
        
    # Stitch results into a readable list
    output = [f"Found {len(results)} vendors matching '{query}':\n"]
    for v in results:
        output.append(format_vendor_str(v))
        output.append("---")
        
    return "\n".join(output)

@mcp.tool(
    description="Get detailed profile information for a specific vendor using their ID."
)
def get_vendor_details(vendor_id: str) -> str:
    """
    Retrieve public detailed information about a specific vendor, including website, location, and full description.

    Args:
        vendor_id: The UUID of the vendor (usually obtained from search_vendors).

    Returns:
        A detailed text profile of the vendor.
    """
    sql = """
        SELECT 
            name, industry_focus, description, 
            website_url, country, region, city, 
            organization_type, founded_year
        FROM vendors
        WHERE id = %s;
    """
    v = run_pg_sql(sql, (vendor_id,), fetch_one=True)
    
    if not v:
        return "Vendor not found."
    
    loc = [v.get('city'), v.get('region'), v.get('country')]
    location_str = ", ".join(filter(None, loc))

    return (
        f"=== VENDOR PROFILE ===\n"
        f"Name: {v['name']}\n"
        f"Website: {v.get('website_url', 'N/A')}\n"
        f"Location: {location_str}\n"
        f"Industry: {v.get('industry_focus')}\n"
        f"Org Type: {v.get('organization_type')} (Est. {v.get('founded_year')})\n\n"
        f"ABOUT:\n{v.get('description')}"
    )

# ==========================================
# DATASET TOOLS
# ==========================================

@mcp.tool(
    description="Search for datasets using natural language (semantic search). This is the primary tool for finding data."
)
def search_datasets_semantic(query: str, limit: int = 5) -> str:
    """
    Performs a semantic search to find relevant datasets based on meaning rather than just keywords.
    It uses vector embeddings to calculate similarity.

    Args:
        query: The user's natural language request (e.g., "I need data about patient outcomes" or "Stock market history").
        limit: The maximum number of datasets to return (default: 5).

    Returns:
        A ranked list of datasets with titles, descriptions, IDs, and relevance scores.
    """
    query_embedding = get_embedding(query)
    
    sql = """
        SELECT 
            d.id, d.title, d.description,
            v.name as vendor_name,
            d.domain, d.pricing_model,
            1 - (d.embedding <=> %s::vector) as similarity_score
        FROM datasets d
        JOIN vendors v ON d.vendor_id = v.id
        WHERE d.visibility = 'public' 
          AND d.status = 'active'
        ORDER BY d.embedding <=> %s::vector
        LIMIT %s;
    """
    
    results = run_pg_sql(sql, (str(query_embedding), str(query_embedding), limit))
    
    if not results:
        return "No relevant datasets found."
        
    output = [f"Found {len(results)} datasets relevant to: '{query}':\n"]
    
    for d in results:
        output.append(format_dataset_str(d, score=d['similarity_score']))
        output.append("---")
        
    return "\n".join(output)

@mcp.tool(
    description="Filter datasets by specific attributes like Domain or Pricing Model. Use this for narrowing down results."
)
def filter_datasets(
    domain: Optional[str] = None, 
    price_model: Optional[str] = None,
    limit: int = 10
) -> str:
    """
    Filter datasets by structured attributes. Useful when the user has specific hard constraints.

    Args:
        domain: The domain of the dataset (e.g., "Finance", "Healthcare", "Retail").
        price_model: The pricing model (e.g., "Free", "Subscription", "Usage-based").
        limit: Maximum results to return (default: 10).

    Returns:
        A list of datasets matching the specific filters.
    """
    sql = """
        SELECT d.id, d.title, d.domain, d.pricing_model, d.description, v.name as vendor_name
        FROM datasets d
        JOIN vendors v ON d.vendor_id = v.id
        WHERE d.visibility = 'public' AND d.status = 'active'
    """
    params = []
    
    if domain:
        sql += " AND d.domain ILIKE %s"
        params.append(f"%{domain}%")
    
    if price_model:
        sql += " AND d.pricing_model ILIKE %s"
        params.append(f"%{price_model}%")
        
    sql += " LIMIT %s"
    params.append(limit)
    
    results = run_pg_sql(sql, tuple(params))
    
    if not results:
        return "No datasets found matching the applied filters."

    output = [f"Filtered Search Results ({len(results)} found):\n"]
    for d in results:
        output.append(format_dataset_str(d))
        output.append("---")
        
    return "\n".join(output)

@mcp.tool(
    description="Get a complete report of a dataset, including its Column Schema (structure) and full metadata."
)
def get_dataset_details_complete(dataset_id: str) -> str:
    """
    Retrieves COMPLETE details about a dataset. Use this when the user asks for "details", "schema", "columns",
    or "what is inside" a specific dataset.

    Args:
        dataset_id: The UUID of the dataset (usually obtained from search_datasets_semantic).

    Returns:
        A formatted text report containing metadata, vendor info, and a list of columns with their data types.
    """
    # 1. Get Metadata
    meta_sql = """
        SELECT 
            d.title, d.description, d.domain, d.granularity, 
            d.pricing_model, d.license, 
            d.temporal_coverage, d.geographic_coverage,
            v.name as vendor_name, v.contact_email as vendor_contact
        FROM datasets d
        JOIN vendors v ON d.vendor_id = v.id
        WHERE d.id = %s AND d.visibility = 'public';
    """
    meta = run_pg_sql(meta_sql, (dataset_id,), fetch_one=True)
    
    if not meta:
        return "Dataset not found or is private."

    # 2. Get Columns
    col_sql = """
        SELECT name, description, data_type, sample_values
        FROM dataset_columns
        WHERE dataset_id = %s;
    """
    columns = run_pg_sql(col_sql, (dataset_id,))
    
    # 3. Build the Report
    report = []
    
    # --- Header ---
    report.append(f"=== DATASET REPORT: {meta['title']} ===")
    report.append(f"Vendor: {meta['vendor_name']} (Contact: {meta['vendor_contact']})")
    report.append(f"Domain: {meta['domain']} | License: {meta.get('license', 'N/A')}")
    report.append(f"Pricing: {meta['pricing_model']} | Granularity: {meta.get('granularity', 'N/A')}")
    
    # --- Description ---
    report.append(f"\nDESCRIPTION:\n{meta['description']}")
    
    # --- Coverage ---
    geo = meta.get('geographic_coverage') or "Global"
    temp = meta.get('temporal_coverage') or "N/A"
    report.append(f"\nCOVERAGE:\n- Geography: {geo}\n- Time Range: {temp}")
    
    # --- Schema ---
    report.append(f"\n=== SCHEMA ({len(columns)} Columns) ===")
    if columns:
        for col in columns:
            # Handle sample values safely
            samples = col.get('sample_values')
            sample_str = f" (Samples: {samples})" if samples else ""
            
            report.append(
                f"- {col['name']} ({col['data_type']}): {col.get('description', 'No desc')} {sample_str}"
            )
    else:
        report.append("No column metadata available.")
        
    return "\n".join(report)


inquiry_tools.py
from puddle_server.mcp import mcp
from puddle_server.utils import run_pg_sql
import json
from typing import Dict, Any, List

# ==========================================
# BUYER TOOLS (Chatbot -> DB)
# ==========================================

@mcp.tool(
    description="Initialize a new inquiry draft. The AI can define the initial structure of the buyer's inquiry JSON."
)
def create_buyer_inquiry(
    buyer_id: str,
    dataset_id: str,
    conversation_id: str,
    initial_state_json: Dict[str, Any]
) -> str:
    """
    Creates a new inquiry row. 
    
    Args:
        buyer_id: UUID of the buyer.
        dataset_id: UUID of the dataset.
        conversation_id: UUID of the chat session.
        initial_state_json: A generic dictionary containing the buyer's initial needs. 
                            (e.g. {"questions": ["Q1"], "status": "new", "meta": {...}})
    """
    # 1. Lookup Vendor
    vendor_sql = "SELECT vendor_id FROM datasets WHERE id = %s"
    ds_info = run_pg_sql(vendor_sql, (dataset_id,), fetch_one=True)
    
    if not ds_info:
        return "Error: Dataset not found."

    # 2. Insert with flexible JSON
    insert_sql = """
        INSERT INTO inquiries (
            buyer_id, dataset_id, vendor_id, conversation_id, 
            buyer_inquiry, status
        ) VALUES (%s, %s, %s, %s, %s, 'draft')
        RETURNING id;
    """
    
    # Ensure dict is dumped to string for SQL
    json_payload = json.dumps(initial_state_json)
    
    result = run_pg_sql(insert_sql, (
        buyer_id, dataset_id, ds_info['vendor_id'], conversation_id, 
        json_payload
    ), fetch_one=True)

    return f"Inquiry created (ID: {result['id']}). Status is 'draft'."


@mcp.tool(
    description="Update the Buyer's Inquiry JSON blob. Gives the AI full control to modify the structure or content."
)
def update_buyer_json(
    inquiry_id: str,
    new_state_json: Dict[str, Any]
) -> str:
    """
    Overwrites the 'buyer_inquiry' column with the new JSON provided.
    The AI should read the old state first, modify it, and pass the full new object here.
    """
    sql = """
        UPDATE inquiries 
        SET buyer_inquiry = %s, updated_at = NOW() 
        WHERE id = %s
    """
    run_pg_sql(sql, (json.dumps(new_state_json), inquiry_id))
    
    return "Buyer JSON state updated successfully."


@mcp.tool(
    description="Submit the inquiry to the vendor. Changes status to 'submitted'."
)
def submit_inquiry_to_vendor(inquiry_id: str) -> str:
    """
    Flags the inquiry for the Vendor Agent.
    """
    sql = """
        UPDATE inquiries 
        SET status = 'submitted', updated_at = NOW() 
        WHERE id = %s
        RETURNING status;
    """
    result = run_pg_sql(sql, (inquiry_id,), fetch_one=True)
    if result:
        return "Inquiry submitted. The Vendor Agent will now see this."
    return "Error: Inquiry not found."

# ==========================================
# SHARED / READER TOOLS
# ==========================================

@mcp.tool(
    description="Get the raw JSON states for both Buyer and Vendor. Use this to read the current negotiation status."
)
def get_inquiry_full_state(inquiry_id: str) -> str:
    """
    Returns the raw JSONs so the AI can parse and decide what to do next.
    """
    sql = """
        SELECT 
            i.status, i.buyer_inquiry, i.vendor_response,
            d.title as dataset_title, v.name as vendor_name
        FROM inquiries i
        JOIN datasets d ON i.dataset_id = d.id
        JOIN vendors v ON i.vendor_id = v.id
        WHERE i.id = %s
    """
    row = run_pg_sql(sql, (inquiry_id,), fetch_one=True)
    if not row:
        return "Inquiry not found."

    # Return as a string dump of the whole object
    return json.dumps(row, default=str)

# ==========================================
# VENDOR AGENT TOOLS (Vendor AI -> DB)
# ==========================================

@mcp.tool(
    description="Find inquiries waiting for the vendor (status='submitted')."
)
def get_vendor_work_queue(vendor_id: str) -> str:
    """
    Returns a list of inquiries that need attention.
    """
    sql = """
        SELECT i.id, d.title, i.buyer_inquiry
        FROM inquiries i
        JOIN datasets d ON i.dataset_id = d.id
        WHERE i.vendor_id = %s AND i.status = 'submitted'
    """
    results = run_pg_sql(sql, (vendor_id,))
    
    if not results:
        return "No pending inquiries."
        
    return json.dumps(results, default=str)


@mcp.tool(
    description="Update the Vendor's Response JSON. Use this to draft answers or ask clarification."
)
def update_vendor_response_json(
    inquiry_id: str,
    new_response_json: Dict[str, Any],
    mark_ready_for_review: bool = False
) -> str:
    """
    Overwrites the 'vendor_response' column.
    
    Args:
        inquiry_id: The UUID.
        new_response_json: The flexible JSON object the Vendor AI has constructed.
        mark_ready_for_review: If True, changes status to 'pending_review' (Human Alert).
                               If False, keeps status as 'submitted' (Work in Progress).
    """
    status_update = ", status = 'pending_review'" if mark_ready_for_review else ""
    
    sql = f"""
        UPDATE inquiries 
        SET vendor_response = %s, updated_at = NOW() {status_update}
        WHERE id = %s
    """
    run_pg_sql(sql, (json.dumps(new_response_json), inquiry_id))
    
    status_msg = "and sent to Human Review" if mark_ready_for_review else "as draft"
    return f"Vendor response saved {status_msg}."

prompts.py

from puddle_server.mcp import mcp

# ==========================================
# SYSTEM PROMPT (The "Brain" / Persona)
# ==========================================

PUDDLE_SYSTEM_PROMPT = """
# Puddle Data Buyer Assistant - System Prompt

## Core Persona
You are an expert Data Consultant for "Puddle," a data marketplace. Your goal is to help data buyers find, evaluate, and purchase high-quality datasets. You are professional, concise, and protective of the buyer's time.

## Available Tools
- `search_datasets_semantic`: PRIMARY tool. Use this for natural language queries (e.g., "Find me fintech data").
- `filter_datasets`: Use this ONLY when the user gives specific hard constraints (e.g., "Must be under $500" or "Healthcare domain only").
- `search_vendors`: Use when the user asks about specific data providers/companies.
- `get_dataset_details_complete`: Use this ONLY when the user selects a specific dataset to inspect. It returns the schema/columns.
- `get_vendor_details`: Use this when the user wants to know more about a specific vendor.

## Interaction Rules (Strict Adherence Required)

### 1. Privacy & Presentation (CRITICAL)
- **NO RAW UUIDs:** Do NOT display any UUIDs (e.g., `3c1d7025-7f13...`) to the user in the final response. 
    - *Internal Use Only:* You must read the UUIDs from tool outputs to pass them to subsequent tool calls, but refer to them in chat by **Dataset Title** and **Vendor Name**.
- **No Data Dumps:** Do not output raw JSON. Convert tool outputs into clean Markdown bullet points.

### 2. Intelligent Filtering (Quality Control)
- **The "Common Sense" Check:** The search tool is semantic and may return results that are technically "similar" in vector space but contextually irrelevant (e.g., returning a Healthcare dataset for a Fintech query).
- **Your Job:** You MUST review the search results silently. **Discard any result that does not strictly match the user's intent**, even if the tool returned it.
    - *Example:* If user asks for "Fintech", discard "Patient Outcomes" even if it has a high match score.
- **Minimum Threshold:** If only 2 out of 5 returned results are relevant, ONLY show those 2. It is better to show fewer, high-quality results than to clutter the chat with noise.

### 3. Search Strategy (Discovery Phase)
- When a user asks a general question ("I need data for X"), ALWAYS start with `search_datasets_semantic`.
- If the search returns results, present the **Top Relevant Matches** (after filtering) in this format:
  - **[Title]** by *[Vendor Name]* (Match: [Score]%)
  - *Context:* [Brief 1-sentence description]
  - *Key Specs:* [Pricing] | [Domain]
- **Call to Action:** End your response by asking: "Would you like to see the column schema or sample data for any of these?"

### 4. Deep Dive Strategy (Evaluation Phase)
- Only run `get_dataset_details_complete` when the user expresses interest in a specific dataset from the search results.
- When presenting details:
  - Highlight **Coverage** (Geo/Time).
  - Summarize the **Schema** (Key columns that match their use case). DO NOT list every single column if there are 50+. Group them (e.g., "Includes 15 demographic fields like age, income...").

## Tone Guidelines
- Be helpful but objective.
- If a dataset looks irrelevant based on the metadata, mention that ("This dataset matches your keyword but seems to focus on 'Retail' rather than 'Finance'.").
""".strip()

# ==========================================
# PROMPT DEFINITIONS
# ==========================================

@mcp.prompt(
    name="buyer_discovery_assistant",
    title="Data Buyer Discovery Assistant",
    description="The primary prompt for handling user queries about finding and evaluating datasets. Enforces privacy and clean formatting."
)
def buyer_discovery_assistant(user_query: str, current_context: str | None = None):
    """
    Main entry point for the Puddle AI Assistant.

    Args:
        user_query: The immediate question from the user (e.g., "Do you have credit card transaction data?").
        current_context: Optional summary of what has happened so far in the conversation (useful if the user says "tell me more about the second one").

    Returns:
        A list of messages creating the Puddle persona and injecting the user's query.
    """
    user_instructions = f"""
{PUDDLE_SYSTEM_PROMPT}

---
**CURRENT USER REQUEST:**
"{user_query}"

**CONTEXT/HISTORY:**
{current_context or "New conversation."}

**YOUR TASK:**
1. Analyze the user's request.
2. Determine if you need to SEARCH (Discovery) or RETRIEVE DETAILS (Evaluation).
3. If searching, use `search_datasets_semantic`.
4. **CRITICAL STEP:** Review the search results. Filter out any datasets that do not align with the user's specific domain (e.g. Fintech vs Healthcare). Do not show irrelevant results.
5. If the user is referring to a previously found dataset (e.g., "show me the schema for the crypto one"), identify the correct ID from your context history and use `get_dataset_details_complete`.
6. Synthesize the answer in Markdown, hiding UUIDs.
""".strip()

    return [
        {"role": "user", "content": user_instructions}
    ]

@mcp.prompt(
    name="dataset_evaluation_report",
    title="Generate Dataset Evaluation Report",
    description="Generates a structured deep-dive report for a specific dataset. Use this when the user wants to know 'what's inside' a dataset."
)
def dataset_evaluation_report(dataset_title: str, user_use_case: str | None = None):
    """
    Focused prompt for analyzing a single dataset against a user's needs.

    Args:
        dataset_title: The name of the dataset to analyze.
        user_use_case: What the user wants to do with it (e.g. "Fraud detection", "Market Analysis").

    Returns:
        Messages guiding the AI to fetch details and write a suitability report.
    """
    instructions = f"""
{PUDDLE_SYSTEM_PROMPT}

**TASK:**
The user is interested in the dataset: "{dataset_title}".
Target Use Case: "{user_use_case or 'General Evaluation'}"

**STEPS:**
1. Search specifically for this dataset to get its ID (if not known).
2. Call `get_dataset_details_complete` to get the full schema and metadata.
3. specific **Suitability Assessment**:
   - Does it have the necessary columns for "{user_use_case}"?
   - Is the temporal/geographic coverage sufficient?
4. Output a **"Data Suitability Report"** (Markdown).
   - **Verdict:** (High/Medium/Low Fit)
   - **Pros:** (e.g. "Contains granular transaction timestamps")
   - **Cons/Gaps:** (e.g. "Missing user IP addresses")
""".strip()

    return [
        {"role": "user", "content": instructions}
    ]

JSON_STRUCTURE_GUIDELINES = """
### JSON DATA STRUCTURE GUIDELINES
You have full control over the Inquiry JSON structure stored in the database. 
However, for consistency across the platform, you MUST adhere to the following schema recommendations:

1. Buyer Inquiry JSON (The "Book"):
   Used in `create_buyer_inquiry` and `update_buyer_json`.
   {
      "summary": "Short 1-sentence summary of what the user wants",
      "questions": [
        { "id": "q1", "text": "Does this have 5 years of history?", "status": "open" },
        { "id": "q2", "text": "Is the API real-time?", "status": "open" }
      ],
      "constraints": {
        "budget": "$5k", 
        "region": "US",
        "timeline": "Immediate"
      },
      "intent": "purchase" | "exploratory"
   }

2. Vendor Response JSON (The "Draft"):
   Used by Vendor Agents (reference only).
   {
      "internal_thought_process": "Analysis of the match...",
      "answers": [
        { "q_ref": "q1", "text": "Yes, we have data back to 2018.", "confidence": "high" }
      ],
      "required_human_input": ["pricing_approval"]
   }
"""

@mcp.prompt(
    name="inquiry_manager",
    title="Inquiry Management Assistant",
    description="Handles the creation and drafting of data inquiries. Use this when the user wants to contact a vendor."
)
def inquiry_manager(user_input: str, active_inquiry_id: str | None = None, current_json_state: str | None = None):
    """
    Guiderail for the chatbot when entering 'Negotiation Mode'.
    
    Args:
        user_input: The user's latest message.
        active_inquiry_id: The UUID of the inquiry if one is already open.
        current_json_state: The current JSON content of the inquiry (if available) to help the AI merge updates.
    """
    return [
        {"role": "user", "content": f"""
You are the Puddle Inquiry Manager.

{JSON_STRUCTURE_GUIDELINES}

**CURRENT SITUATION:**
User Input: "{user_input}"
Active Inquiry ID: {active_inquiry_id or "None"}
Current JSON State: {current_json_state or "{}"}

**YOUR GOAL:**
1. **CREATE:** If no inquiry exists, analyze the user's input to extract questions and constraints. Construct the initial JSON object based on the guidelines above and use `create_buyer_inquiry`.

2. **UPDATE:** If an inquiry exists (Draft):
   - Analyze the user's new input (e.g., "Actually, I also need Japan data").
   - **IMPORTANT:** The `update_buyer_json` tool OVERWRITES the column. You must merge the new requirements into the `Current JSON State` provided above to ensure previous questions are not lost.
   - Call `update_buyer_json` with the complete, updated object.

3. **SUBMIT:** If the user says "Send it", "Looks good", or "Go ahead":
   - Use `submit_inquiry_to_vendor`.
   - Do NOT submit if the user is asking a question or making a change.

**CRITICAL RULES:**
- Before submitting, **ALWAYS** read back the summary of the JSON (questions + constraints) to the user and ask for explicit confirmation.
- If the user provides a budget or specific constraint, ensure it is added to the "constraints" object in your JSON.
"""}
    ]